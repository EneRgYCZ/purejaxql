# this is adapted from Atari configuration (with a slightly higher exploration eps), may get better results with some tuning
ALG_NAME: "pqn_rnn"
TOTAL_TIMESTEPS: 1e9
TOTAL_TIMESTEPS_REAL: 1e9 # will be used for decay functions, in case you want to test for less timesteps and keep decays same
NUM_ENVS: 1024 # parallel environments
MEMORY_WINDOW: 4 # steps of previous episode added in the rnn training horizon
NUM_STEPS: 128 # steps per environment in each update
EPS_START: 0.1
EPS_FINISH: 0.005
EPS_DECAY: 0.2 # ratio of total updates
NUM_MINIBATCHES: 16 # minibatches per epoch
NUM_EPOCHS: 2 # minibatches per epoch
NORM_INPUT: True
NORM_TYPE: "layer_norm" # layer_norm or batch_norm
HIDDEN_SIZE: 512
NUM_LAYERS: 4
LR: 0.0001
MAX_GRAD_NORM: 1.
LR_LINEAR_DECAY: False
REW_SCALE: 1.
GAMMA: 0.99
LAMBDA: 0.95

# env specific
ENV_NAME: "Craftax-Symbolic-v1"
USE_OPTIMISTIC_RESETS: True
OPTIMISTIC_RESET_RATIO: 16
LOG_ACHIEVEMENTS: False

# evaluation
TEST_DURING_TRAINING: True 
TEST_INTERVAL: 0.01 # in terms of total updates
TEST_NUM_ENVS: 512
TEST_NUM_STEPS: 1000
EPS_TEST: 0. # 0 for greedy policy
