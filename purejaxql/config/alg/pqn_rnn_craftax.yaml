# this is adapted from Atari configuration (with a slightly higher exploration eps), may get better results with some tuning
ALG_NAME: "pqn_rnn_2rnns"
TOTAL_TIMESTEPS: 1e9
TOTAL_TIMESTEPS_REAL: 1e9 # will be used for decay functions, in case you want to test for less timesteps and keep decays same
NUM_ENVS: 512 # parallel environments
MEMORY_WINDOW: 0 # steps of previous episode added in the rnn training horizon
NUM_STEPS: 64 # steps per environment in each update
EPS_START: 0.5
EPS_FINISH: 0.001
EPS_DECAY: 0.1 # ratio of total updates
NUM_MINIBATCHES: 4 # minibatches per epoch
NUM_EPOCHS: 4 # minibatches per epoch
NORM_INPUT: True
NORM_TYPE: "batch_norm" # layer_norm or batch_norm
HIDDEN_SIZE: 512
NUM_LAYERS: 4
NUM_RNN_LAYERS: 2
ADD_LAST_ACTION: False
LR: 0.00005
MAX_GRAD_NORM: 1.
LR_LINEAR_DECAY: True
REW_SCALE: 1.
GAMMA: 0.99
LAMBDA: 0.9

# env specific
ENV_NAME: "Craftax-Symbolic-v1"
USE_OPTIMISTIC_RESETS: True
OPTIMISTIC_RESET_RATIO: 16
LOG_ACHIEVEMENTS: False

# evaluation
TEST_DURING_TRAINING: True 
TEST_INTERVAL: 0.01 # in terms of total updates
TEST_NUM_ENVS: 512
TEST_NUM_STEPS: 1000
EPS_TEST: 0.001 # 0 for greedy policy
